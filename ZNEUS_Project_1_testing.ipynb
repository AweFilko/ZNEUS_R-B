{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Imports and data fetching**"
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "49d7682549ff49ecb51dddb1fe9771a4",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 6210,
    "execution_start": 1762381083946,
    "source_hash": "27f3d23a",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score\n",
    "import itertools\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from torch.optim import RMSprop"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **Decision about graph printing**"
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "graphs = input(\"Print graphs? [y/n]: \").lower() == 'y'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "240a56c9550f47748b84b029bffcd08b",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 1406,
    "execution_start": 1762381090206,
    "source_hash": "509cbf29",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# fetch dataset\n",
    "magic_gamma_telescope = fetch_ucirepo(id=159)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "DS_feature = magic_gamma_telescope.data.features\n",
    "DS_target = magic_gamma_telescope.data.targets\n",
    "\n",
    "# metadata\n",
    "print(magic_gamma_telescope.metadata)\n",
    "\n",
    "# variable information\n",
    "print(magic_gamma_telescope.variables)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "edc028fb2e9343dfa23ef0089d6605c5",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": []
   },
   "source": [
    "### yay"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **Data exploration**"
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "032c6501bef04f7a970a6b02aab9135a",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 0,
    "execution_start": 1762381091677,
    "source_hash": "a843147c",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "#data exloration\n",
    "print(DS_feature.describe())\n",
    "\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "print(DS_target.describe())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### **Duplicates detection**"
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "1450c923b8c0432c8b9a9fc5449b7275",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 4,
    "execution_start": 1762381091742,
    "source_hash": "a1b4e5bf",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "print(\"Duplicates in features:\", DS_feature.duplicated().sum())\n",
    "\n",
    "#DS_feature = DS_feature.drop_duplicates()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### **Anomalies detection and handling**"
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "a037faf503c1491abadb39555e653c78",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 1,
    "execution_start": 1762381091796,
    "source_hash": "19725dbe",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "#anomalies detected\n",
    "def count_anomalies(df, column, normal_min, normal_max):\n",
    "    mask_normal = (df[column] >= normal_min) & (df[column] <= normal_max)\n",
    "    anomalies = (~mask_normal).sum()\n",
    "    return anomalies\n",
    "\n",
    "print(f\"fLength: \", count_anomalies(DS_feature, 'fLength', 0, 1000))\n",
    "print(f\"fWidth: \", count_anomalies(DS_feature, 'fWidth', 0, 1000))\n",
    "print(f\"fSize: \", count_anomalies(DS_feature, 'fSize', 0, 1000))\n",
    "print(f\"fAlpha: \", count_anomalies(DS_feature, 'fAlpha', 0, 180))\n",
    "print(f\"fDist: \", count_anomalies(DS_feature, 'fDist', 0, 1000))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### **Outlier detection and handling**"
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "6419c31c18a24c20876caae40a13a603",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 16,
    "execution_start": 1762381091858,
    "source_hash": "31642093",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "features = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long', 'fM3Trans', 'fAlpha', 'fDist']\n",
    "\n",
    "\n",
    "for col in features:\n",
    "    Q1 = DS_feature[col].quantile(0.25)\n",
    "    Q3 = DS_feature[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "\n",
    "    # set outliers to NaN\n",
    "\n",
    "    DS_feature[col] = DS_feature[col].where(DS_feature[col].between(lower, upper), np.nan)\n",
    "    #DS_feature.loc[(DS_feature[col] < lower) | (DS_feature[col] > upper), col] = np.nan\n",
    "\n",
    "\n",
    "# --- Missingness and outlriers handling ---\n",
    "threshold = 30\n",
    "\n",
    "missing_pct = DS_feature.isna().mean() * 100\n",
    "to_drop_cols = missing_pct[missing_pct > threshold ].index.tolist()\n",
    "to_impute_cols = missing_pct[(missing_pct >= 0.1) & (missing_pct <= threshold)].index.tolist()\n",
    "\n",
    "print(f'Missing percentages:\\n{missing_pct}')\n",
    "print(f'Number of outliers: {(DS_feature.isna().sum().sum())}')\n",
    "\n",
    "dropped_cols = []\n",
    "imputed_cols = []\n",
    "\n",
    "# drop columns with more than 30% (threshold) missing values\n",
    "for col in to_drop_cols:\n",
    "    if col in DS_feature.columns:\n",
    "        DS_feature = DS_feature.drop(columns=[col])\n",
    "        dropped_cols.append(col)\n",
    "\n",
    "# for columns with 0.1-30% missing -> impute\n",
    "\n",
    "for col in to_impute_cols:\n",
    "    if col not in DS_feature.columns:\n",
    "        continue\n",
    "    # checks if the columnâ€™s dtype is numeric\n",
    "    if pd.api.types.is_numeric_dtype(DS_feature[col]):\n",
    "        median_val = DS_feature[col].median(skipna=True)\n",
    "        # replaces missing values with the median (less sensetive to outliners than mean)\n",
    "        # inplace=True means the changes are applied directly to the DataFrame\n",
    "        DS_feature[col].fillna(median_val, inplace=True)\n",
    "        imputed_cols.append((col, \"median\", median_val))\n",
    "    else:\n",
    "        # for non-numeric, impute with mode if available\n",
    "        try:\n",
    "            mode_val = DS_feature[col].mode(dropna=True)\n",
    "            if len(mode_val) > 0:\n",
    "                mode_val = mode_val.iloc[0]\n",
    "                DS_feature[col].fillna(mode_val, inplace=True)\n",
    "                imputed_cols.append((col, \"mode\", mode_val))\n",
    "            else:\n",
    "                # if no mode, fill with placeholder\n",
    "                DS_feature[col].fillna(\"missing\", inplace=True)\n",
    "                imputed_cols.append((col, \"placeholder\", \"missing\"))\n",
    "        except Exception as e:\n",
    "            DS_feature[col].fillna(\"missing\", inplace=True)\n",
    "            imputed_cols.append((col, \"placeholder\", \"missing\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "28843e64347243e3982e94109d10a4e2",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 2,
    "execution_start": 1762381091936,
    "source_hash": "2765ae3e",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "print(\"Missingness handling summary:\")\n",
    "print(f'Dropped columns (missingness > {threshold}%): {len(dropped_cols)}\\n{dropped_cols}')\n",
    "print(f'Imputed columns (missingness between 5% and {threshold}%): {len(imputed_cols)}\\n{imputed_cols}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###### **QQ plot and hist plot for normal distribution check**"
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "23c7852c40254b448d36130d3ae1aece",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 1693,
    "execution_start": 1762381092007,
    "source_hash": "67160c10",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "features = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long', 'fM3Trans', 'fAlpha', 'fDist']\n",
    "if graphs:\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15,8))\n",
    "\n",
    "    for ax, feature in zip(axes.flatten(), features):\n",
    "        stats.probplot(DS_feature[feature], dist=\"norm\", plot=ax)\n",
    "        ax.set_title(f\"QQ Plot: {feature}\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('no')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "f78d696e95534fb2a4bda696ec7e1bef",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 4138,
    "execution_start": 1762381094827,
    "source_hash": "84bede73",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "if graphs:\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15,8))\n",
    "\n",
    "    for ax, feature in zip(axes.flatten(), features):\n",
    "        sns.histplot(DS_feature[feature], kde=True, ax=ax)\n",
    "        ax.set_title(f\"Distribution of {feature} by class\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No plotting, just scheming\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###### **Boxplot for outlier check**"
  },
  {
   "metadata": {
    "cell_id": "4a8937268b014aeb82b9b5ed434aa0ec",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 1019,
    "execution_start": 1762381093756,
    "source_hash": "7e72cc4c",
    "vscode": {
     "languageId": "python"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if graphs:\n",
    "    DS_feature.plot(kind='box', subplots=True,\n",
    "                                  layout=(2, 5), figsize=(15, 10), sharex=False)\n",
    "    plt.suptitle(\"Boxplots for all features\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"EE\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### **Class distribution**"
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "a639e13b1c03444aa791a935314a580c",
    "deepnote_cell_type": "code",
    "deepnote_table_loading": false,
    "deepnote_table_state": {
     "cellFormattingRules": [],
     "columnDisplayNames": [],
     "columnOrder": [
      "class"
     ],
     "conditionalFilters": [],
     "filters": [],
     "hiddenColumnIds": [],
     "pageIndex": 0,
     "pageSize": 100,
     "sortBy": [],
     "wrappedTextColumnIds": []
    },
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 209,
    "execution_start": 1762381099086,
    "source_hash": "bfd4f834",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "if DS_target['class'].dtype == 'object':\n",
    "        DS_target['class'] = DS_target['class'].map({'g': 1, 'h': 0})\n",
    "\n",
    "if graphs:\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.countplot(x='class', data=DS_target, palette='Set2', hue='class')\n",
    "    plt.title(\"Distribution of Classes (Gamma vs Hadron)\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"nah\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### **Corelation Matrix**"
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "7df7c403d8aa409db525f3685a2a99a1",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 458,
    "execution_start": 1762381099346,
    "source_hash": "e26a7281",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "if graphs:\n",
    "    df = DS_feature.copy()\n",
    "    df['class'] = DS_target\n",
    "\n",
    "    df.drop_duplicates()\n",
    "\n",
    "    if df['class'].dtype == 'object':\n",
    "        df['class'] = df['class'].map({'g': 1, 'h': 0})\n",
    "\n",
    "    corr_matrix = df.corr(numeric_only=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "    plt.title(\"Correlation Matrix - MAGIC Gamma Telescope Dataset\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Dont feel like it!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **Data transformation and split**"
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "5eebc1860c3a4bad857a320a3240caa0",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 116,
    "execution_start": 1762381099867,
    "source_hash": "72c65255",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "DS_feature_scaled = DS_feature.copy()\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "DS_feature = pd.DataFrame(imputer.fit_transform(DS_feature), columns=DS_feature.columns)\n",
    "\n",
    "cols_power  = ['fLength', 'fWidth', 'fSize', 'fAlpha','fAsym', 'fM3Long', 'fM3Trans']\n",
    "\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "DS_feature_scaled[cols_power] = pt.fit_transform(DS_feature[cols_power])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### **Distribution check after the transformation**"
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "10fbbd52c47c496f8aea514288ebf290",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 4230,
    "execution_start": 1762381100038,
    "source_hash": "e34339c0",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "if graphs:\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15,8))\n",
    "\n",
    "    for ax, feature in zip(axes.flatten(), features):\n",
    "        sns.histplot(DS_feature_scaled[feature], kde=True, ax=ax)\n",
    "        ax.set_title(f\"Distribution of {feature} by class\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Meeh!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "e147246e38dc430787f8d9f43d9292c3",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 1314,
    "execution_start": 1762381104346,
    "source_hash": "99f58093",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "if graphs:\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15,8))\n",
    "\n",
    "    for ax, feature in zip(axes.flatten(), features):\n",
    "        stats.probplot(DS_feature_scaled[feature], dist=\"norm\", plot=ax)\n",
    "        ax.set_title(f\"QQ Plot: {feature}\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Think abut the gas prices!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### **Train/Test split**"
  },
  {
   "metadata": {
    "cell_id": "83607ae5bca246e39b7b67b035b7d14c",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 1,
    "execution_start": 1762381105706,
    "source_hash": "cbf65db7",
    "vscode": {
     "languageId": "python"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X = DS_feature_scaled\n",
    "y = DS_target[\"class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=14, stratify=y\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **Feature importance**"
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "d48002a8aa224eb6abaff18ba778c424",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 2168,
    "execution_start": 1762381105756,
    "source_hash": "2d6eae5d",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "\n",
    "\n",
    "remaining_feat = list(features)\n",
    "selected_feat = []\n",
    "current_score = 0.0\n",
    "best_new_score = 0.0\n",
    "\n",
    "progress = []\n",
    "\n",
    "while remaining_feat:\n",
    "    score_with_candidates = []\n",
    "\n",
    "    for candidate in remaining_feat:\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train[selected_feat + [candidate]], y_train)\n",
    "        score = model.score(X_test[selected_feat + [candidate]], y_test)\n",
    "        score_with_candidates.append((score, candidate))\n",
    "\n",
    "    score_with_candidates.sort()\n",
    "    best_new_score, best_candidate = score_with_candidates.pop()\n",
    "\n",
    "    if current_score < best_new_score:\n",
    "        remaining_feat.remove(best_candidate)\n",
    "        selected_feat.append(best_candidate)\n",
    "        current_score = best_new_score\n",
    "        # print(f\"Adding {best_candidate} with score {current_score}\")\n",
    "\n",
    "        progress.append((len(selected_feat), best_candidate, current_score))\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(f\"Forward Selection Result: {selected_feat};\")\n",
    "print(f\"Number of Selected Features: {len(selected_feat)};\")\n",
    "print(f\"Final Validation Score: {current_score:.4f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### **Forward selection R^2**"
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "2694f92301e9467391a65b9bbb32ca1d",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 124,
    "execution_start": 1762381107976,
    "source_hash": "24fd5769",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "if graphs:\n",
    "    steps, feats, scores = zip(*progress)\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(steps, scores, marker='o')\n",
    "    plt.title(\"Forward Selection Performance\")\n",
    "    plt.xlabel(\"Number of Selected Features\")\n",
    "    plt.ylabel(\"Validation RÂ²\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No Forward Selection Result!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "6cc13ae0486f4f8d8d33b05966837903",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 1,
    "execution_start": 1762381108156,
    "source_hash": "42c22ea8",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "remaining_feat = list(X_train.columns)\n",
    "current_score = 0.0\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train[remaining_feat], y_train)\n",
    "best_score = model.score(X_test[remaining_feat], y_test)\n",
    "\n",
    "scores_progress = [(len(remaining_feat), best_score)]\n",
    "print(f\"Initial RÂ² with all features: {best_score:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### **Backward selection R^2**"
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "0fce5a14702d4ee7892cb8975ff794d8",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 2257,
    "execution_start": 1762381108266,
    "source_hash": "f9b9a367",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "while len(remaining_feat) > 1:\n",
    "    scores_with_candidates = []\n",
    "    \n",
    "    for feature in remaining_feat:\n",
    "        reduced_features = [f for f in remaining_feat if f != feature]\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train[reduced_features], y_train)\n",
    "        score = model.score(X_test[reduced_features], y_test)\n",
    "        scores_with_candidates.append((score, feature))\n",
    "    \n",
    "    scores_with_candidates.sort(reverse=True)\n",
    "    best_new_score, worst_feature = scores_with_candidates[0]\n",
    "\n",
    "    if best_new_score < best_score - 0.001:\n",
    "        break\n",
    "\n",
    "    remaining_feat.remove(worst_feature)\n",
    "    best_score = max(best_new_score, best_score)\n",
    "    scores_progress.append((len(remaining_feat), best_new_score))\n",
    "    print(f\"Removed {worst_feature}, new RÂ²: {best_new_score:.4f}\")\n",
    "\n",
    "print(\"Backward Elimination Result:\")\n",
    "print(f\"Selected features: {remaining_feat};\")\n",
    "print(f\"Number of selected features: {len(remaining_feat)}\")\n",
    "print(f\"Final Validation RÂ²: {best_score:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "4a62f02e2aa44309a31e7d182a429521",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 86,
    "execution_start": 1762381110586,
    "source_hash": "1d520ba3",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "if graphs:\n",
    "    steps, scores = zip(*scores_progress)\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(steps, scores, marker=\"o\", linestyle=\"-\", color=\"brown\")\n",
    "    plt.title(\"Backward Elimination Performance\")\n",
    "    plt.xlabel(\"Number of Remaining Features\")\n",
    "    plt.ylabel(\"Validation RÂ²\")\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Nothing to do!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **Trainning setup**"
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "#choose what to run\n",
    "run = int(input(\n",
    "    \"Run config (enter number): \"\n",
    "    \"[0] default training; [1] early stopping; [2] grid search; [3] grid + early stopping\\n\"\n",
    "))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "6581e6b581604385a6b5cc98212e027c",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 0,
    "execution_start": 1762381110717,
    "source_hash": "2d5f30da",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "#single run params\n",
    "\n",
    "test_split = 0.2\n",
    "batch_size = 512\n",
    "random_state = 14\n",
    "in_features = None # later int the following cells is split where it sets in features auto (no need 2 set)\n",
    "learning_rate = 1e-3\n",
    "epochs = 500\n",
    "dropout_rate = 0.2\n",
    "weight_decay = 1e-4\n",
    "pos = 1.8\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# grid search config\n",
    "\n",
    "config = {\n",
    "    \"lr\": [0.001, 5e-4],\n",
    "    \"hidden_dim\": [64, 128],\n",
    "    \"dropout_p\": [0.05, 0.1],\n",
    "    \"epochs\": [10],\n",
    "    \"batch_size\": [256, 512],\n",
    "    \"weight_decay\": [1e-5, 1e-3],\n",
    "    \"pos_weight\": [1,0.8,2]\n",
    "}\n",
    "\n",
    "# early stopping settings\n",
    "\n",
    "monitor = \"accuracy\"\n",
    "mode = \"max\"\n",
    "patience = 50 # epochs\n",
    "best_metric = -float(\"inf\") if mode == \"max\" else float(\"inf\")\n",
    "wait = 0\n",
    "\n",
    "# best model path for later possibility of loading it back\n",
    "\n",
    "best_model_path = \"best_model.pt\"\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **Feature setting and final split**"
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "d67493bdaa814522b53b9849542a131d",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 26,
    "execution_start": 1762381110776,
    "source_hash": "c53b20a6",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "T_features = DS_feature[['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long', 'fAlpha', 'fDist']].values\n",
    "T_labels = DS_target['class'].values\n",
    "\n",
    "in_features = T_features.shape[1] #here is the in feature set from above initialization\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    T_features, T_labels, test_size=test_split, random_state=random_state\n",
    ")\n",
    "\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "val_ds = TensorDataset(X_val, y_val)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **Loading data**"
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "3691b28d82864d6194282901135b0add",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 0,
    "execution_start": 1762381110846,
    "source_hash": "de9b1c9e",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **Model structure**"
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "d4c42ec2528f4725a6334433d6ddb589",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 0,
    "execution_start": 1762381110896,
    "source_hash": "d40fea1c",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "class MagicalNet(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=128, dropout_p=dropout_rate):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.drop1 = nn.Dropout(dropout_p)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_dim, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.drop2 = nn.Dropout(dropout_p)\n",
    "\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.fc1(x)))\n",
    "        x = self.drop1(x)\n",
    "        x = F.leaky_relu(self.bn2(self.fc2(x)))\n",
    "        x = self.drop2(x)\n",
    "        return self.fc3(x)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "ac625fe4eb874e1aa551242906a2b018",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 1,
    "execution_start": 1762381139309,
    "source_hash": "faff18f8",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "\n",
    "model = MagicalNet(in_dim=in_features, hidden_dim=64, dropout_p=dropout_rate)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos]))\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "#optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.001)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "4d621f7e1f334750b17c19d37631f4de",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 5355,
    "execution_start": 1762381154776,
    "source_hash": "407fcfdc",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "wandb.login()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "08dedee5f05b4bacb4832ef7a8ac46bf",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 6478,
    "execution_start": 1762381166170,
    "source_hash": "5069e8df",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "if run == 0 or run == 1:\n",
    "    wandb.init(project=\"ZNEUS_R&B\", name=\"gamma_classification\",\n",
    "        config={\n",
    "            \"epochs\": epochs,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"dropout_p\": dropout_rate,\n",
    "            \"in_features\": in_features,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"architecture\": \"MagicalNet\"\n",
    "        })"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "738e4cf5d0de40a19ba10d7aa96bb064",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 2833,
    "execution_start": 1762381328716,
    "source_hash": "d5efa74a",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "if run == 0:\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb, yb.unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(val_loader.dataset)\n",
    "        #scheduler.step()\n",
    "        #print(f\"Current LR: {optimizer.param_groups[0]['lr']}\")\n",
    "        wandb.log({\"train_loss\": epoch_loss, \"epoch\": epoch + 1})\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} â€” loss: {epoch_loss:.4f}\")\n",
    "else:\n",
    "    print(\"Not me!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "ff7060bbd4e14b0480c8b53b4526b6b9",
    "deepnote_cell_type": "code",
    "execution_context_id": "a36accf4-c15c-4590-81b2-dc9e829279fa",
    "execution_millis": 0,
    "execution_start": 1762381363907,
    "source_hash": "c8330e5d",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "if run == 0:\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            logits = model(xb)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).long()\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_targets.append(yb.unsqueeze(1).long())\n",
    "\n",
    "    all_preds = torch.cat(all_preds).squeeze().numpy()\n",
    "    all_targets = torch.cat(all_targets).squeeze().numpy()\n",
    "    accuracy = (all_preds == all_targets).mean()\n",
    "\n",
    "    precision = precision_score(all_targets, all_preds)\n",
    "    recall = recall_score(all_targets, all_preds)\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "    wandb.log({\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"confusion matrix\": cm,\n",
    "    })\n",
    "else:\n",
    "    print(\"Not me either!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "if run == 1:\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # -------- training --------\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb, yb.unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        #scheduler.step()\n",
    "        train_epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        wandb.log({\"train_loss\": train_epoch_loss, \"epoch\": epoch + 1})\n",
    "        #print(f\"Current LR: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        # -------- validation / evaluation --------\n",
    "\n",
    "        model.eval()\n",
    "        probs_list = []\n",
    "        targets_list = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                logits = model(xb)\n",
    "                probs = torch.sigmoid(logits).cpu().numpy().squeeze()\n",
    "                probs_list.append(probs)\n",
    "                targets_list.append(yb.numpy().squeeze())\n",
    "\n",
    "        all_probs = np.concatenate(probs_list)\n",
    "        all_targets = np.concatenate(targets_list)\n",
    "        all_preds = (all_probs > 0.5).astype(int)\n",
    "\n",
    "        # compute metrics\n",
    "        accuracy = (all_preds == all_targets).mean()\n",
    "        precision = precision_score(all_targets, all_preds, zero_division=0)\n",
    "        recall = recall_score(all_targets, all_preds, zero_division=0)\n",
    "        f1 = f1_score(all_targets, all_preds, zero_division=0)\n",
    "        cm = confusion_matrix(all_targets, all_preds)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} â€” train_loss: {train_epoch_loss:.4f}  val_f1: {f1:.4f}\")\n",
    "        print(f\"Acc: {accuracy:.4f}, Prec: {precision:.4f}, Rec: {recall:.4f}\")\n",
    "        print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "        wandb.log({\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1,\n",
    "            \"confusion matrix\": cm,\n",
    "        })\n",
    "\n",
    "        # -------- early stopping logic --------\n",
    "\n",
    "        if monitor == \"f1\":\n",
    "            current = f1\n",
    "        elif monitor == \"precision\":\n",
    "            current = precision\n",
    "        elif monitor == \"recall\":\n",
    "            current = recall\n",
    "        elif monitor == \"accuracy\":\n",
    "            current = accuracy\n",
    "        else:\n",
    "            raise ValueError(\"Unknown monitor metric\")\n",
    "\n",
    "        improved = (current > best_metric) if mode == \"max\" else (current < best_metric)\n",
    "        if improved:\n",
    "            best_metric = current\n",
    "            wait = 0\n",
    "            torch.save(model.state_dict(), best_model_path)   # save best weights\n",
    "            print(f\"  New best {monitor}: {best_metric:.4f}. Saved model.\")\n",
    "        else:\n",
    "            wait += 1\n",
    "            print(f\"  No improvement for {wait} epoch(s).\")\n",
    "\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered. Restoring best model.\")\n",
    "            break\n",
    "\n",
    "\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            logits = model(xb)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).long()\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_targets.append(yb.unsqueeze(1).cpu())\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    all_preds = torch.cat(all_preds).squeeze().numpy()\n",
    "    all_targets = torch.cat(all_targets).squeeze().numpy()\n",
    "\n",
    "    # --- Compute metrics ---\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_targets, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_targets, all_preds, zero_division=0)\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "\n",
    "    # --- Print results ---\n",
    "    print(\"Best Model Validation Metrics:\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-score:  {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "else:\n",
    "    print(\"Nothing to do.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "if run == 2:\n",
    "    # --- Create all possible combinations\n",
    "    param_combinations = list(itertools.product(*config.values()))\n",
    "\n",
    "    # --- Iterate through all combinations\n",
    "    for combo in param_combinations:\n",
    "        params = dict(zip(config.keys(), combo))\n",
    "        print(f\"\\nRunning grid search with params: {params}\")\n",
    "\n",
    "        wandb.init(project=\"ZNEUS_R&B\", config=params, name=\"GC_grid_search\")\n",
    "        config = wandb.config\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size= config.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_ds, batch_size= config.batch_size, shuffle=False)\n",
    "\n",
    "        model = MagicalNet(in_dim=in_features, hidden_dim=config.hidden_dim, dropout_p=config.dropout_p)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([config.pos_weight]))\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "\n",
    "        # --- Training loop (your original)\n",
    "        for epoch in range(config.epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb, yb.unsqueeze(1)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            wandb.log({\"train_loss\": epoch_loss, \"epoch\": epoch + 1})\n",
    "            #print(f\"Epoch {epoch+1}/{config.epochs} â€” loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # --- Validation accuracy check\n",
    "        model.eval()\n",
    "        probs_list = []\n",
    "        targets_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                logits = model(xb)\n",
    "                probs = torch.sigmoid(logits).cpu().numpy().squeeze()\n",
    "                probs_list.append(probs)\n",
    "                targets_list.append(yb.numpy().squeeze())\n",
    "\n",
    "        all_probs = np.concatenate(probs_list)\n",
    "        all_targets = np.concatenate(targets_list).astype(int)\n",
    "        all_preds = (all_probs > 0.5).astype(int)\n",
    "\n",
    "        accuracy = accuracy_score(all_targets, all_preds)\n",
    "        precision = precision_score(all_targets, all_preds)\n",
    "        recall = recall_score(all_targets, all_preds)\n",
    "        f1 = f1_score(all_targets, all_preds)\n",
    "        cm = confusion_matrix(all_targets, all_preds)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1-score: {f1:.4f}\")\n",
    "        #print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "        wandb.log({\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1,\n",
    "            \"confusion_matrix\": cm,\n",
    "        })\n",
    "else:\n",
    "    print(\"No grid search.\\n\"\n",
    "          \" ðŸ¥º\\n\"\n",
    "          \"ðŸ‘‰ðŸ‘ˆ\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "if run == 3:\n",
    "    # --- Hyperparameter grid\n",
    "    param_combinations = list(itertools.product(*config.values()))\n",
    "\n",
    "\n",
    "    # --- Grid search\n",
    "    for combo in param_combinations:\n",
    "        params = dict(zip(config.keys(), combo))\n",
    "        print(f\"\\nRunning grid search with params: {params}\")\n",
    "\n",
    "\n",
    "        run = wandb.init(project=\"ZNEUS_R&B\",name = \"GC_Grid_search_w_es\", config=params, reinit=True)\n",
    "        config = wandb.config\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size= config.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_ds, batch_size= config.batch_size, shuffle=False)\n",
    "\n",
    "        # build model / criterion / optimizer\n",
    "        model = MagicalNet(in_dim=in_features, hidden_dim=config.hidden_dim, dropout_p=config.dropout_p)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "\n",
    "        # early stopping state\n",
    "        best_epoch = -1\n",
    "        best_metrics = None\n",
    "\n",
    "        # training + validation per epoch (so we can early-stop)\n",
    "        for epoch in range(config.epochs):\n",
    "            # -------- training --------\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb, yb.unsqueeze(1)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "            train_epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "            # -------- validation --------\n",
    "            model.eval()\n",
    "            probs_list = []\n",
    "            targets_list = []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    logits = model(xb)\n",
    "                    probs = torch.sigmoid(logits).cpu().numpy().squeeze()\n",
    "                    probs_list.append(probs)\n",
    "                    targets_list.append(yb.numpy().squeeze())\n",
    "\n",
    "            all_probs = np.concatenate(probs_list)\n",
    "            all_targets = np.concatenate(targets_list).astype(int)\n",
    "            all_preds = (all_probs > 0.5).astype(int)\n",
    "\n",
    "            # compute metrics\n",
    "            prec = precision_score(all_targets, all_preds, zero_division=0)\n",
    "            rec = recall_score(all_targets, all_preds, zero_division=0)\n",
    "            f1 = f1_score(all_targets, all_preds, zero_division=0)\n",
    "            acc = accuracy_score(all_targets, all_preds)\n",
    "            cm = confusion_matrix(all_targets, all_preds)\n",
    "\n",
    "            if monitor == \"f1\":\n",
    "                current = f1\n",
    "            elif monitor == \"precision\":\n",
    "                current = prec\n",
    "            elif monitor == \"recall\":\n",
    "                current = rec\n",
    "            elif monitor == \"accuracy\":\n",
    "                current = acc\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown monitor metric: {monitor}\")\n",
    "\n",
    "            # logging\n",
    "            wandb.log({\n",
    "                \"train_loss\": train_epoch_loss,\n",
    "                \"accuracy\": acc,\n",
    "                \"precision\": prec,\n",
    "                \"recall\": rec,\n",
    "                \"f1_score\": f1,\n",
    "                \"epoch\": epoch + 1\n",
    "            })\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{config.epochs} â€” train_loss: {train_epoch_loss:.4f}  val_f1: {f1:.4f}\")\n",
    "\n",
    "            # early stopping decision\n",
    "            improved = (current > best_metric) if mode == \"max\" else (current < best_metric)\n",
    "            if improved:\n",
    "                best_metric = current\n",
    "                wait = 0\n",
    "                best_epoch = epoch + 1\n",
    "                best_metrics = {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"confusion_matrix\": cm}\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                print(f\"  New best {monitor}: {best_metric:.4f} (saved)\")\n",
    "            else:\n",
    "                wait += 1\n",
    "                print(f\"  No improvement for {wait} epoch(s) (patience={patience})\")\n",
    "\n",
    "            if wait >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        if best_metrics is not None:\n",
    "            model.load_state_dict(torch.load(best_model_path, map_location=\"cpu\"))\n",
    "            model.eval()\n",
    "\n",
    "            probs_list = []\n",
    "            targets_list = []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    logits = model(xb)\n",
    "                    probs = torch.sigmoid(logits).cpu().numpy().squeeze()\n",
    "                    probs_list.append(probs)\n",
    "                    targets_list.append(yb.numpy().squeeze())\n",
    "\n",
    "            all_probs = np.concatenate(probs_list)\n",
    "            all_targets = np.concatenate(targets_list).astype(int)\n",
    "            all_preds = (all_probs > 0.5).astype(int)\n",
    "\n",
    "            prec = precision_score(all_targets, all_preds, zero_division=0)\n",
    "            rec = recall_score(all_targets, all_preds, zero_division=0)\n",
    "            f1 = f1_score(all_targets, all_preds, zero_division=0)\n",
    "            acc = accuracy_score(all_targets, all_preds)\n",
    "            cm = confusion_matrix(all_targets, all_preds)\n",
    "\n",
    "            # log final best-run metrics\n",
    "            wandb.log({\n",
    "                \"best_val_accuracy\": acc,\n",
    "                \"best_val_precision\": prec,\n",
    "                \"best_val_recall\": rec,\n",
    "                \"best_val_f1\": f1\n",
    "            })\n",
    "            print(\"BEST validation metrics (loaded best model):\")\n",
    "            print(f\"Accuracy:  {acc:.4f}\")\n",
    "            print(f\"Precision: {prec:.4f}\")\n",
    "            print(f\"Recall:    {rec:.4f}\")\n",
    "            print(f\"F1-score:  {f1:.4f}\")\n",
    "            print(\"Confusion matrix:\\n\", cm)\n",
    "        else:\n",
    "            print(\"No improvement recorded during run; no best model saved.\")\n",
    "else:\n",
    "    print(\"Why am I here?. Just to suffer!\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "wandb.finish()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# model = MagicalNet(in_dim=in_features, hidden_dim=64)\n",
    "#\n",
    "# # Load the saved weights\n",
    "# model.load_state_dict(torch.load('model_87.pt'))\n",
    "# model.eval()\n",
    "#\n",
    "# # Example input â€” your real data goes here\n",
    "# new_input = [[1.46773678e+00, 2.22794825e+00 , 2.22794825e+00 , 1.68000000e-01, 1.01100000e-01,\n",
    "#               1.68377205e+00, 2.20081444e+005, -1.79772516e-01, 2.31902800e+02]]\n",
    "#\n",
    "# # Convert to tensor\n",
    "# x = torch.tensor(new_input, dtype=torch.float32)\n",
    "#\n",
    "#\n",
    "# with torch.no_grad():\n",
    "#     logit = model(x)\n",
    "#     prob = torch.sigmoid(logit).item()      # convert to probability\n",
    "#     pred = int(prob > 0.5)\n",
    "#\n",
    "# if pred == 1:\n",
    "#     print(f\"Prediction: Class 1 (probability: {prob:.3f})\")\n",
    "# else:\n",
    "#     print(f\"Prediction: Class 0 (probability: {prob:.3f})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# idx = DS_target[DS_target['class'] == 0].index[0]\n",
    "#\n",
    "# feature_row = DS_feature_scaled.loc[idx, ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long', 'fAlpha', 'fDist']]\n",
    "# target_val = DS_target.loc[idx, 'class']\n",
    "#\n",
    "# print(\"Features:\", feature_row.values)\n",
    "# print(\"Target:\", target_val)\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "ef5db1a2a3a2440eb025491bbfab5f2d",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": "4",
 "nbformat_minor": "0"
}
